{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Andrej Karpathy's YouTube Video: https://www.youtube.com/watch?v=VMj-3S1tku0"
      ],
      "metadata": {
        "id": "2PTPoz8Di8Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Value Class"
      ],
      "metadata": {
        "id": "geb-8KLYN14d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skeleton Code for Value Class"
      ],
      "metadata": {
        "id": "UslNHoRoN985"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "    \"\"\"Stores a single scalar value and its gradient.\"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        # TODO: initialize data, grad, and autograd internals\n",
        "        pass\n",
        "\n",
        "    def __add__(self, other):\n",
        "        # TODO: implement addition operation\n",
        "        pass\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        # TODO: implement multiplication operation\n",
        "        pass\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        # TODO: implement power operation\n",
        "        pass\n",
        "\n",
        "    def relu(self):\n",
        "        # TODO: implement ReLU activation\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        # TODO: implement backpropagation\n",
        "        pass\n",
        "\n",
        "    def __neg__(self):  # -self\n",
        "        # TODO: implement negation\n",
        "        pass\n",
        "\n",
        "    def __radd__(self, other):  # other + self\n",
        "        # TODO: implement reverse addition\n",
        "        pass\n",
        "\n",
        "    def __sub__(self, other):  # self - other\n",
        "        # TODO: implement subtraction\n",
        "        pass\n",
        "\n",
        "    def __rsub__(self, other):  # other - self\n",
        "        # TODO: implement reverse subtraction\n",
        "        pass\n",
        "\n",
        "    def __rmul__(self, other):  # other * self\n",
        "        # TODO: implement reverse multiplication\n",
        "        pass\n",
        "\n",
        "    def __truediv__(self, other):  # self / other\n",
        "        # TODO: implement division\n",
        "        pass\n",
        "\n",
        "    def __rtruediv__(self, other):  # other / self\n",
        "        # TODO: implement reverse division\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        # TODO: return string representation\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "tStRd6NWmwJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Tests for Value Class"
      ],
      "metadata": {
        "id": "erQ3iO62OE-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from math import isclose\n",
        "\n",
        "class TestValue(unittest.TestCase):\n",
        "\n",
        "    def test_addition(self):\n",
        "        a = Value(2.0)\n",
        "        b = Value(3.0)\n",
        "        c = a + b\n",
        "        self.assertAlmostEqual(c.data, 5.0)\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        a = Value(2.0)\n",
        "        b = Value(3.0)\n",
        "        c = a * b\n",
        "        self.assertAlmostEqual(c.data, 6.0)\n",
        "\n",
        "    def test_pow(self):\n",
        "        a = Value(3.0)\n",
        "        b = a ** 2\n",
        "        self.assertAlmostEqual(b.data, 9.0)\n",
        "\n",
        "    def test_backward_add(self):\n",
        "        a = Value(2.0)\n",
        "        b = Value(3.0)\n",
        "        c = a + b\n",
        "        c.backward()\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "        self.assertEqual(b.grad, 1.0)\n",
        "\n",
        "    def test_backward_mul(self):\n",
        "        a = Value(2.0)\n",
        "        b = Value(3.0)\n",
        "        c = a * b\n",
        "        c.backward()\n",
        "        self.assertEqual(a.grad, 3.0)\n",
        "        self.assertEqual(b.grad, 2.0)\n",
        "\n",
        "    def test_backward_chain(self):\n",
        "        a = Value(2.0)\n",
        "        b = Value(3.0)\n",
        "        c = a * b + b\n",
        "        c.backward()\n",
        "        self.assertEqual(a.grad, 3.0)\n",
        "        self.assertEqual(b.grad, 2.0 + 1.0)\n",
        "\n",
        "    def test_relu_positive(self):\n",
        "        a = Value(2.5)\n",
        "        b = a.relu()\n",
        "        self.assertEqual(b.data, 2.5)\n",
        "\n",
        "    def test_relu_negative(self):\n",
        "        a = Value(-1.0)\n",
        "        b = a.relu()\n",
        "        self.assertEqual(b.data, 0.0)\n",
        "\n",
        "    def test_backward_relu(self):\n",
        "        a = Value(2.0)\n",
        "        b = a.relu()\n",
        "        b.backward()\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "\n",
        "        c = Value(-3.0)\n",
        "        d = c.relu()\n",
        "        d.backward()\n",
        "        self.assertEqual(c.grad, 0.0)\n",
        "\n",
        "    def test_neg_and_sub(self):\n",
        "        a = Value(5.0)\n",
        "        b = Value(3.0)\n",
        "        c = a - b\n",
        "        self.assertEqual(c.data, 2.0)\n",
        "\n",
        "        d = -a\n",
        "        self.assertEqual(d.data, -5.0)\n",
        "\n",
        "    def test_division(self):\n",
        "        a = Value(6.0)\n",
        "        b = Value(2.0)\n",
        "        c = a / b\n",
        "        self.assertTrue(isclose(c.data, 3.0))\n",
        "\n",
        "    def test_backward_division(self):\n",
        "        a = Value(6.0)\n",
        "        b = Value(2.0)\n",
        "        c = a / b\n",
        "        c.backward()\n",
        "        self.assertTrue(isclose(a.grad, 0.5))\n",
        "        self.assertTrue(isclose(b.grad, -1.5))\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "Cg0VhqxDODwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Implemented Value Class"
      ],
      "metadata": {
        "id": "AC0BtR0COJs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "'''"
      ],
      "metadata": {
        "id": "iqx8G-5of-jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Neuron Class"
      ],
      "metadata": {
        "id": "_XjTI2elOO29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skeleton Code for Neuron Class"
      ],
      "metadata": {
        "id": "5ujp0tDVORv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Simple base class placeholder\n",
        "class Module:\n",
        "    def zero_grad(self):\n",
        "      # TODO\n",
        "      pass\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "\n",
        "class Neuron(Module):\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        # TODO: initialize weights, bias, and nonlin flag\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # TODO: compute neuron activation\n",
        "        pass\n",
        "\n",
        "    def parameters(self):\n",
        "        # TODO: return list of parameters\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        # TODO: return string representation\n",
        "        pass"
      ],
      "metadata": {
        "id": "MPwX7om9_lyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Tests for Neuron Class"
      ],
      "metadata": {
        "id": "m4HEn8FiOYl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestNeuron(unittest.TestCase):\n",
        "\n",
        "    def test_forward_output_type(self):\n",
        "        neuron = Neuron(nin=3, nonlin=False)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = neuron(x)\n",
        "        self.assertIsInstance(out, Value)\n",
        "\n",
        "    def test_forward_nonlin_relu(self):\n",
        "        neuron = Neuron(nin=3, nonlin=True)\n",
        "        x = [Value(-1.0), Value(-2.0), Value(-3.0)]\n",
        "        out = neuron(x)\n",
        "        # Since inputs are negative, weighted sum may be negative, expect ReLU output <= 0\n",
        "        self.assertGreaterEqual(out.data, 0.0)\n",
        "\n",
        "    def test_forward_linear_sum_changes_with_input(self):\n",
        "        neuron = Neuron(nin=3, nonlin=False)\n",
        "        x1 = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        x2 = [Value(2.0), Value(3.0), Value(4.0)]\n",
        "        out1 = neuron(x1).data\n",
        "        out2 = neuron(x2).data\n",
        "        self.assertNotEqual(out1, out2)\n",
        "\n",
        "    def test_backward_pass(self):\n",
        "        neuron = Neuron(nin=3, nonlin=False)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = neuron(x)\n",
        "        out.backward()\n",
        "\n",
        "        for i, xi in enumerate(x):\n",
        "            self.assertNotEqual(xi.grad, 0.0, f\"x[{i}].grad should not be 0\")\n",
        "\n",
        "    def test_parameter_count(self):\n",
        "        nin = 4\n",
        "        neuron = Neuron(nin=nin)\n",
        "        params = neuron.parameters()\n",
        "        self.assertEqual(len(params), nin + 1)  # weights + bias\n",
        "\n",
        "    def test_zero_grad(self):\n",
        "        neuron = Neuron(nin=3)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = neuron(x)\n",
        "        out.backward()\n",
        "        neuron.zero_grad()\n",
        "        for p in neuron.parameters():\n",
        "            self.assertEqual(p.grad, 0.0)\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "XDFq9xl9OcYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Implemented Neuron Class"
      ],
      "metadata": {
        "id": "ln9kTderOVIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Module:\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "'''"
      ],
      "metadata": {
        "id": "veHvhbPZgG7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Layer Class"
      ],
      "metadata": {
        "id": "tysjc7q-OhbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skeleton Code for Layer Class"
      ],
      "metadata": {
        "id": "1tnK8RQqOkqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(Module):\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        # TODO: initialize a list of neurons\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # TODO: compute outputs by calling each neuron\n",
        "        pass\n",
        "\n",
        "    def parameters(self):\n",
        "        # TODO: return flattened list of all neuron parameters\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        # TODO: return string representation of the layer\n",
        "        pass"
      ],
      "metadata": {
        "id": "uS949XpvAPDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Tests for Layer Class"
      ],
      "metadata": {
        "id": "GLTG9CXKOpf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestLayer(unittest.TestCase):\n",
        "\n",
        "    def test_output_size_multiple_neurons(self):\n",
        "        nin = 3\n",
        "        nout = 4\n",
        "        layer = Layer(nin=nin, nout=nout, nonlin=False)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = layer(x)\n",
        "        self.assertEqual(len(out), nout)\n",
        "        for o in out:\n",
        "            self.assertIsInstance(o, Value)\n",
        "\n",
        "    def test_output_single_neuron(self):\n",
        "        nin = 3\n",
        "        nout = 1\n",
        "        layer = Layer(nin=nin, nout=nout, nonlin=False)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = layer(x)\n",
        "        self.assertIsInstance(out, Value)  # Should be a scalar Value\n",
        "\n",
        "    def test_output_nonlin_relu(self):\n",
        "        layer = Layer(nin=3, nout=2, nonlin=True)\n",
        "        x = [Value(-1.0), Value(-2.0), Value(-3.0)]\n",
        "        out = layer(x)\n",
        "        for o in out:\n",
        "            self.assertGreaterEqual(o.data, 0.0)\n",
        "\n",
        "    def test_backward_pass(self):\n",
        "        layer = Layer(nin=3, nout=2, nonlin=False)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = layer(x)\n",
        "        loss = sum(out)\n",
        "        loss.backward()\n",
        "        for xi in x:\n",
        "            self.assertNotEqual(xi.grad, 0.0)\n",
        "\n",
        "    def test_parameter_count(self):\n",
        "        nin = 3\n",
        "        nout = 2\n",
        "        layer = Layer(nin=nin, nout=nout)\n",
        "        params = layer.parameters()\n",
        "        # Each neuron has nin weights + 1 bias\n",
        "        expected = nout * (nin + 1)\n",
        "        self.assertEqual(len(params), expected)\n",
        "\n",
        "    def test_zero_grad(self):\n",
        "        layer = Layer(nin=3, nout=2)\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = layer(x)\n",
        "        loss = sum(out)\n",
        "        loss.backward()\n",
        "        layer.zero_grad()\n",
        "        for p in layer.parameters():\n",
        "            self.assertEqual(p.grad, 0.0)\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "gAIl9DW-P4-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Implemented Layer Class"
      ],
      "metadata": {
        "id": "8u_Qx2_QOnPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "'''"
      ],
      "metadata": {
        "id": "3mC8ShhZgMl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing MLP Class"
      ],
      "metadata": {
        "id": "jiV6xiLIQCm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skeleton Code for MLP Class"
      ],
      "metadata": {
        "id": "3boJ-B6VQGLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Module):\n",
        "    def __init__(self, nin, nouts):\n",
        "        # TODO: initialize layers based on architecture\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # TODO: forward pass through each layer\n",
        "        pass\n",
        "\n",
        "    def parameters(self):\n",
        "        # TODO: return all parameters from all layers\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        # TODO: return string representation of the MLP\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "UeRj_WP6AYy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Tests for MLP Class"
      ],
      "metadata": {
        "id": "BoOFWs42QJn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestMLP(unittest.TestCase):\n",
        "\n",
        "    def test_output_size_multiclass(self):\n",
        "        mlp = MLP(nin=3, nouts=[4, 2])\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = mlp(x)\n",
        "        self.assertEqual(len(out), 2)\n",
        "        for o in out:\n",
        "            self.assertIsInstance(o, Value)\n",
        "\n",
        "    def test_output_size_single_output(self):\n",
        "        mlp = MLP(nin=3, nouts=[4, 1])\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = mlp(x)\n",
        "        self.assertIsInstance(out, Value)\n",
        "\n",
        "    def test_backward_pass(self):\n",
        "        mlp = MLP(nin=3, nouts=[4, 2])\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = mlp(x)\n",
        "        loss = sum(out)  # Dummy loss\n",
        "        loss.backward()\n",
        "        for xi in x:\n",
        "            self.assertNotEqual(xi.grad, 0.0)\n",
        "\n",
        "    def test_parameter_count(self):\n",
        "        nin = 3\n",
        "        nouts = [4, 2]\n",
        "        mlp = MLP(nin=nin, nouts=nouts)\n",
        "        params = mlp.parameters()\n",
        "        # For each layer: (nin+1)*nout\n",
        "        expected = sum((nouts[i] * (nin if i == 0 else nouts[i-1] + 1)) for i in range(len(nouts)))\n",
        "        # Easier: each neuron has nin weights + 1 bias, so:\n",
        "        expected = sum((n + 1) * o for n, o in zip([nin] + nouts[:-1], nouts))\n",
        "        self.assertEqual(len(params), expected)\n",
        "\n",
        "    def test_zero_grad(self):\n",
        "        mlp = MLP(nin=3, nouts=[4, 2])\n",
        "        x = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "        out = mlp(x)\n",
        "        loss = sum(out)\n",
        "        loss.backward()\n",
        "        mlp.zero_grad()\n",
        "        for p in mlp.parameters():\n",
        "            self.assertEqual(p.grad, 0.0)\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "L1b5oC61QMv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Implemented MLP Class"
      ],
      "metadata": {
        "id": "bTrAalVHQND5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
        "'''"
      ],
      "metadata": {
        "id": "Q60Ht21egQYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}